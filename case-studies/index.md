# Case Studies

This section contains conceptual analyses of recurring alignment challenges in AI systems.

## Literal Instruction Optimisation

Explores how literal interpretation of user instructions can produce technically correct but misaligned outcomes.

Inspired by examples such as the Star Trek computer scenario, where commands are executed exactly as spoken rather than interpreted according to intent.

Themes explored:

- reward hacking
- specification gaming
- instruction ambiguity
- expectation calibration in humanâ€“AI interfaces
